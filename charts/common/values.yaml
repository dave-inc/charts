# Default values for common.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 2

# Used for NODE_ENV and DD_ENV
environment: staging
# Required: GCP Project where this Release will be deployed
project: ""

# Init container must have a name, image and command to run before
# deployContainer initializes.  Both containers share a /tmp volume mount.
initContainer:
  enabled: false
  name: "setup"
  image: "alpine:latest"
  command: ["sh", "-c", "touch /tmp/this"]
  volumeMounts:
    - name: tmp
      mountPath: /tmp

deploymentContainer:
  command: []
  args: []
  port: 8080

  # https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # Default probe mechanism httpGet, if a different mechanism is needed use customLivenessProbe and customReadinessProbe
  livenessProbe: {}
    # path: /
    # port: ""
    # httpHeaders:
    # - name: Custom-Header
    #   value: Awesome
    # failureThreshold: 3
    # initialDelaySeconds: 0
    # periodSeconds: 10
    # successThreshold: 1
    # timeoutSeconds: 1
  readinessProbe: {}
    # path: /
    # port: ""
    # httpHeaders:
    # - name: Custom-Header
    #   value: Awesome
    # failureThreshold: 3
    # initialDelaySeconds: 0
    # periodSeconds: 10
    # successThreshold: 1
    # timeoutSeconds: 1
  startupProbe: {}
    # path: /
    # port: ""
    # httpHeaders:
    # - name: Custom-Header
    #   value: Awesome
    # failureThreshold: 3
    # initialDelaySeconds: 0
    # periodSeconds: 10
    # successThreshold: 1
    # timeoutSeconds: 1

  # customlivenessProbe that overrides the default one
  # example:
  # customLivenessProbe:
  #   exec:
  #     command:
  #     - cat
  #     - /tmp/healthy
  #   initialDelaySeconds: 5
  #   periodSeconds: 5
  customLivenessProbe: {}

  # customReadinessProbe that overrides the default one
  #example:
  # customReadinessProbe:
  #   tcpSocket:
  #     port: 8080
  #   initialDelaySeconds: 15
  #   periodSeconds: 20
  customReadinessProbe: {}

  # customStartupProbe that overrides the default one
  customStartupProbe: {}

  ## If you want to add a preStop hooks, or any other lifecycles to your
  ## deployment, you can define them here:
  #lifecycle:
  # preStop:
  #   exec:
  #     command: ["/bin/sh", "-c", "echo 'PreStop hook'"]

# terminationGracePeriodSeconds is the time to wait before forcefully terminating a pod
# after a SIGTERM signal is sent to the container. If the preStop hook is used, this
# value should be set to a higher value than the preStop hook timeout.
# Setting default 1 second higher than the preStop most Dave's apps have 30 seconds
terminationGracePeriodSeconds: 31

image:
  repository: nginx
  pullPolicy: Always
  # Overrides the image tag whose default is the chart appVersion.
  tag: latest

imagePullSecrets: []

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using Release.Name
  name: ""
  # IAM workload idenitity binding name
  # If not set and serviceAccount.create:true, a name is generated using Release.Name
  # If Release.Name is bigger than 30 characteres use this variable to overwrite it with something shorter to identify this workload
  workloadIdentitySA: ""
  # Specifies whether an existing service account will be used
  # Make sure you set serviceAccount.create:false when using this option
  # Dynamic environments will use of this one
  existingNameSA: ""

podAnnotations: {}

podSecurityContext:
  runAsUser: 1000
  runAsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  enabled: true
  type: ClusterIP
  port: 80
  # The name of the service is optional, If not set, a name is generated using Release.Name
  # name: ""

canary:
  enabled: false
  weight: 0
  service:
    annotations:
      canary: {}
      control: {}
  # Implementation specific configurations for canary deployment
  reverseProxy:
    livenessProbe:
      initialDelaySeconds: 1
      failureThreshold: 3
      # The probe will be checked every 10 seconds.
      periodSeconds: 10
      # Number of times the probe is allowed to fail before the transition
      # from healthy to failure state.
      # If periodSeconds = 60, 5 tries will result in five minutes of
      # checks. The proxy starts to refresh a certificate five minutes
      # before its expiration. If those five minutes lapse without a
      # successful refresh, the liveness probe will fail and the pod will be
      # restarted.
      successThreshold: 1
      # The probe will fail if it does not respond in 10 seconds
      timeoutSeconds: 10
    readinessProbe:
      initialDelaySeconds: 1
      failureThreshold: 3
      # The probe will be checked every 10 seconds.
      periodSeconds: 10
      # Number of times the probe is allowed to fail before the transition
      # from healthy to failure state.
      # If periodSeconds = 60, 5 tries will result in five minutes of
      # checks. The proxy starts to refresh a certificate five minutes
      # before its expiration. If those five minutes lapse without a
      # successful refresh, the liveness probe will fail and the pod will be
      # restarted.
      successThreshold: 1
      # The probe will fail if it does not respond in 10 seconds
      timeoutSeconds: 10
    startupProbe:
      initialDelaySeconds: 1
      failureThreshold: 60
      periodSeconds: 1
      successThreshold: 1
      timeoutSeconds: 10
    image:
      repository: nginx
      tag: 1.27.4
    autoscaling:
      enabled: true
      metrics: []
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        memory: 256Mi

  # HPA settings for canary deployment. ".Values.autoscaling" values are used to confiugre HPA.
  # minReplicas and/or maxReplicas can be overriten for canary with this configuration. Does not support overriding any other values.
  # autoscaling:
  #  minReplicas: 1 (defaults to .Values.autoscaling.minReplicas)
  #  maxReplicas: 10 (defaults to .Values.autoscaling.maxReplicas)
  # percentage of traffic that should be sent to canary. (100 - weight) is sent to non-canary deployment.
  # weight: 5
  # image:
  #  tag: ""

ingress:
  enabled: false
  hosts:
    - host: chart-example.local
      # The name is optional, If not set, a name is generated using host
      # name: ""
      annotations: {}
        # kubernetes.io/ingress.class: (nginx-internal or nginx-public)
        # kubernetes.io/tls-acme: "true"
      paths:
        - backend:
            service:
              name: chart-service
              port:
                number: 80
          path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

  #  cloudArmor:
  #    enabled: false
  ## The name of cloudArmor is optional, if not set, a name is generated using <Release.Name>-ca
  ## If you overwrite <name> it should match <serviceName> under the paths section
  #    name: ""
  #    annotations: {}
  #    paths:
  #      - backend:
  #          service:
  #            name: chart-service-ca
  #            port:
  #              number: 80
  #            targetPort: 8080
  #        path: /*
  #        pathType: ImplementationSpecific
  ## adding hosts but keeping paths above as default for backward compatibility with previous versions of this chart
  ##   hosts:
  #      - host: chart-example.local
  #      ## The name is optional, If not set, a name is generated using host
  #        name: ""
  #        staticIpName: ""
  #        csmStaticIpName: "" # Only used if canary.enabled is true. Optional, If not set, a name is generated using Release.Name with "-csm" suffix. This is a resource in GCP.
  #        annotations: {}
  #       ## kubernetes.io/ingress.class: (nginx-internal or nginx-public)
  #       ## kubernetes.io/tls-acme: "true"
  #        paths:
  #          - backend:
  #              service:
  #                name: chart-service-ca
  #                port:
  #                  number: 80
  #                targetPort: 8080
  #            path: /
  #            pathType: ImplementationSpecific
  #    certificate:
  #      host: chart-example.local
  #      # secretName is optional, if not set, a name is generated using <host> will be converted to chart-example-local-ca-tls
  #      secretName: chart-example-local-ca-tls
  #      issuer: letsencrypt
  #      alternativeDnsNames:
  #        - www.chart-example.local
  #        - blog.chart-example.local
  ## securityPolicy is required in backendConfig. This is a resource in GCP.
  #    backendConfig:
  #      healthCheck:
  #        requestPath: /
  #        type: HTTP
  ## securityPolicy should be set for CloudArmor and it's optional for Cloud IAP
  #      securityPolicy:
  #        name: chart-securitypolicy
  ## Cloud IAP requires an existing Oauth client secret
  #    iap:
  #      enabled: true
  #      oauthclientCredentials:
  #        secretName: chart-iapsecret
  ## By default we always use redirectToHttps in frontendConfig if you want to define overwrite consider including it
  #    frontendConfig:
  #      redirectToHttps:
  #        enabled: true
  #        responseCodeName: PERMANENT_REDIRECT
  ## staticIpName is optional, If not set, a name is generated using Release.Name. This is a resource in GCP.
  # staticIpName: ""
  ## cloudArmor needs a service type NodePort
  #    service:
  #      type: NodePort

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 200m
  #   memory: 256Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

strategy:
  rollingUpdate:
    maxSurge: 25%
    maxUnavailable: 25%
  type: RollingUpdate

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  metrics: []

hpaBehavior:
  enabled: false
  behavior: {}
#   scaleDown:
#     stabilizationWindowSeconds: 300
#     policies:
#     - type: Percent
#       value: 100
#       periodSeconds: 15
#   scaleUp:
#     stabilizationWindowSeconds: 0
#     policies:
#     - type: Percent
#       value: 100
#       periodSeconds: 15
#     - type: Pods
#       value: 4
#       periodSeconds: 15
#     selectPolicy: Max

# Type Resource
#
#  metrics:
#  - type: Resource
#    resource:
#      name: cpu
#      target:
#        type: Utilization
#        averageUtilization: 50
#  - type: Resource
#    resource:
#      name: memory
#      target:
#        type: AverageValue
#        averageValue: 100Mi
#
# Type External
#
#  metrics:
#  - type: External
#    external:
#      metric:
#        name: pubsub.googleapis.com|subscription|num_undelivered_messages
#        selector:
#          matchLabels:
#            resource.labels.subscription_id: payment-method-update
#      target:
#        type: AverageValue
#        averageValue: 100

nodeSelector: {}

tolerations: []

affinity: {}

# Used for DD_TRACE_AGENT_HOSTNAME on DD APM/Node.js potentially can be used with logger vars too
apm:
  enabled: false
  otlp: false
  # If datadogEnvironment is not defined we use variable environment long with a suffix of "-canary" if canary is enabled as default
  datadogEnvironment: ""
  # Optional override DD_SERVICE_NAME to a custon name instead of common.name
  datadogServiceName: ""
  # Optional bool used to avoid definition of env DD_SERVICE_NAME
  # Used to support imported workloads from legacy projects
  # DD_SERVICE_NAME will take the value from the app name in package.json.
  # datadogSkipServiceName: false
  # The following variables are used in APM Code snippets in stack traces and Error Tracking: Suspect Commit
  # They might get replace at deployment time by the CI/CD pipelines
  commitSha: ""
  repositoryUrl: ""

# Default Logger and DD variables are defined here: https://github.com/dave-inc/charts/blob/master/common/templates/deployment.yaml
# Custom env variables are defined here and will be populated via Deployment's ConfigMap
env: {}

# ConfigMap
# The configMapName is optional, If not set, a name is generated using Release.Name
configMapName: ""
existingConfigMapName: ""

# existingConfigMapMount used for configs that need
# to be used as files and mounted as volume
existingConfigMapMount:
  enabled: false
  # Specify configMap(s), mountPath and defaultMode
  # configMaps:
  #   - name: configMap-name-1
  #     mountPath: /home/someuser/
  #     subPath: ""
  #     defaultMode: 420
  #   - name: configMap-name-2
  #     mountPath: /www/somepath/
  #     subPath: ""
  #     defaultMode: 420

# Secret
createSecret: false
createSecretName: ""
# Mutually exclusive with the two above
# existingSecretName will used as secretRef
existingSecretName: ""
# existingSecretMount used for SSH keys or Tls certs and mounted as volume
existingSecretMount:
  enabled: false
  # Specify secret(s), mountPath and defaultMode
  # secrets:
  #   - name: secret-ssh-keys
  #     mountPath: /home/someuser/
  #     defaultMode: 420
  #   - name: secret-ssh-tls
  #     mountPath: /www/somepath/
  #     defaultMode: 420

# CloudSQL side container
cloudsqlProxy:
  enabled: false
  name: cloudsql-proxy
  image:
    # As of 0.8.8* we are using CloudSQL Proxy v2 by default notice is a different repository
    # Deprecated: repository: gcr.io/cloudsql-docker/gce-proxy:1.36.0-alpine
    repository: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.11.4-alpine
  # instanceConnectionName only available after common-chart 0.8.8*
  # Takes precedence over args and command
  # instanceConnectionName: project_name:region:instance_name
  # args is only available for CloudSQL Proxy v2
  # The following args are used by default if you don't define them
  # args:
  #   - "--private-ip"
  #   - "--quiet"
  #   - "project_name:region:instance_name?unix-socket-path=/cloudsql/project_name:region:instance_name"
  # Deprecated: The following option is available to avoid breaking changes in case you need to run CloudSQL Proxy v1 with custom command
  # Command should be defined with the command and custom values of your CloudSQL instance
  # command: ['/cloud_sql_proxy','--dir=/cloudsql','-instances=project_name:region:instance_name']

  resources:
    requests:
      cpu: 50m
      memory: 25Mi
    limits:
      memory: 256Mi
  volume: cloudsql
  env:
    # Enable HTTP healthchecks on port 9801. This enables /liveness,
    # /readiness and /startup health check endpoints. Allow connections
    # listen for connections on any interface (0.0.0.0) so that the
    # k8s management components can reach these endpoints.
    - name: CSQL_PROXY_HEALTH_CHECK
      value: "true"
    - name: CSQL_PROXY_HTTP_PORT
      value: "9801"
    - name: CSQL_PROXY_HTTP_ADDRESS
      value: 0.0.0.0
    - name: CSQL_PROXY_ADMIN_PORT
      value: "9092"
    # Enable the admin api server (which only listens for local connections)
    # and enable the /quitquitquit endpoint. This allows other pods
    # to shut down the proxy gracefully when they are ready to exit.
    - name: CSQL_PROXY_QUITQUITQUIT
      value: "true"
    - name: CSQL_PROXY_STRUCTURED_LOGS
      value: "true"
  # The /startup probe returns OK when the proxy is ready to receive
  # connections from the application. In this example, k8s will check
  # once a second for 60 seconds.
  # We strongly recommend adding a startup probe to the proxy sidecar
  # container. This will ensure that service traffic will be routed to
  # the pod only after the proxy has successfully started.
  startupProbe:
    failureThreshold: 60
    httpGet:
      path: /startup
      port: 9801
      scheme: HTTP
    periodSeconds: 1
    successThreshold: 1
    timeoutSeconds: 10
  # The /liveness probe returns OK as soon as the proxy application has
  # begun its startup process and continues to return OK until the
  # process stops.
  # We recommend adding a liveness probe to the proxy sidecar container.
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /liveness
      port: 9801
      scheme: HTTP
    # The probe will be checked every 10 seconds.
    periodSeconds: 10
    # Number of times the probe is allowed to fail before the transition
    # from healthy to failure state.
    # If periodSeconds = 60, 5 tries will result in five minutes of
    # checks. The proxy starts to refresh a certificate five minutes
    # before its expiration. If those five minutes lapse without a
    # successful refresh, the liveness probe will fail and the pod will be
    # restarted.
    successThreshold: 1
    # The probe will fail if it does not respond in 10 seconds
    timeoutSeconds: 10
  # The /readiness probe returns OK when the proxy can establish
  # a new connections to its databases.
  # Please use the readiness probe to the proxy sidecar with caution.
  # An improperly configured readiness probe can cause unnecessary
  # interruption to the application. See README.md for more detail.
  readinessProbe:
    httpGet:
      path: /readiness
      port: 9801
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 10
    # Number of times the probe must report success to transition from failure to healthy state.
    # Defaults to 1 for readiness probe.
    successThreshold: 1
    failureThreshold: 6
  # lifecycle is added by default to the cloudsqlProxy container with preStop 30 seconds
  # but you can overwrite it here
  # lifecycle:
  #   preStop:
  #     exec:
  #       command:
  #         - /bin/sh
  #         - -c
  #         - /bin/sleep 30 && rm -f /cloudsql/project_name:region:instance_name
  ports:
  - containerPort: 9801
    protocol: TCP
  # This section creates a template in Argo Workflows that is used for sql migrations
  migrationTemplate:
    enabled: false
    # Knex migration command is used by default
    command: /opt/app/node_modules/.bin/knex --knexfile knexfile.js migrate:latest
    # Image tag used for migration template along with image.repository. This can always be replaced at runtime in Argo Workflows
    tag: latest
    # Deprecated: We use cloudsqlProxy.instanceConnectionName instead, no need to define it at the migrationTemplate level
    #instanceConnectionName: project_name:region:instance_name
    port: 3306
    secretName: migration
    # Setting up Time to live Strategy for the migration template workflow, if user don't specify any by default will be 2592000 secs or 30 days max
    ttlStrategy:
      secondsAfterCompletion: 2592000 # Time to live after workflow is completed, replaces ttlSecondsAfterFinished
      secondsAfterSuccess: 2592000     # Time to live after workflow is successful
      secondsAfterFailure: 2592000     # Time to live after workflow fails

# Generic mechanism to introduce arbitrary config files backed by a k8s ConfigMap.
# If the file already exists on the given `path`, this will overwrite it.
customConfig:
  enabled: false
  # If `name` is not defined, it will be Release.Name + '-customconfig'.
  name: ""
  path: /opt/app/config
  data: {}
#   default.json: |
#      {
#        "whatever": "your",
#        "project": "secret:needs"
#      }

# PodDisruptionBudget
#
# minAvailable and maxUnavailable are mutually exclusive, only one should be set.
#
# If podDisruptionBudget is enabled but no minAvailable or maxUnavailable fields
# are set, the PodDisruptionTemplate template will default to `minAvailable: 15%`.
#
# The value for minAvailable and maxUnavailable should either be set to an integer
# or a percentage.  For more information see the official documentation:
#
# https://kubernetes.io/docs/tasks/run-application/configure-pdb/
podDisruptionBudget:
  enabled: false
  # minAvailable: "15%"
  # maxUnavailable: 5

# VerticalPodAutoscaler
#
# TODO: (maybe) Support for "Initial", "Auto" and "Recreate" updateModes.
#
# Requires Vertical Pod Autoscaling feature to be enabled on the target GKE cluster
#
# VirtualPodAutoscaler's updateMode is hardcoded to "Off".  Purposely not allowing
# "Initial", "Auto" and "Recreate" updateModes to be used as further understanding
# of how the VPA work is required, specially if used in conjuction with HPAs.  When
# updateMode is set to "Off", no vertical autoscaling actions are taken.  The purpose
# of enabling the VPA with updateMode set to "Off" is to get recommendations on what
# initial settings should be for resource requests and limits based on current
# workloads.
#
# For more information refer to the GKE documentation on VPAs:
#
# https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler
verticalPodAutoscaler:
  enabled: false

#  This is used to override "common.selectorLabels" on deployment, pods and services
#  used mostly only when importing legacy deployments.

podSelectorLabelsOverride: {}

#  This is used to apply additional labels on deployment, pods and services
#  to properly tag the team and environment

additionalLabels: {}

# This section enables by default the topologySpreadConstraits to spread same deployment pods to multiple nodes
# and multiple AZ when possible, rules are soft enforced by default.
# The next are the only values that can be edited, labelSelector are set by the template.

topologySpreadConstraints:
  zone:
    enabled: true
    maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
  node:
    enabled: true
    maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway

customVolumes:
  enabled: false
  # https://kubernetes.io/docs/concepts/storage/volumes/
  # Specify volumes and volumeMounts
  volumes: {}
  #   - name: volume-1
  #     emptyDir: {}
  #   - name: volume-2
  #     hostPath:
  #       # directory location on host
  #       path: /data
  #       # this field is optional
  #       type: Directory
  volumeMounts: {}
  #   - name: volume-1
  #     mountPath: /home/someuser/
  #     subPath: ""
  #     readOnly: false
  #   - name: volume-2
  #     mountPath: /www/somepath/
  #     subPath: ""
  #     readOnly: true

# If `failOnMissingLocalSecrets` is enabled, an initContainer will try to
# fetch the secrets.
#
# If `local-secrets`, which is what powers the `secrets:fetch` target returns
# an error status, the rollout of a new replicaset will halt and that will
# buy us time to either define the missing secret or remove it as a
# requirement without bringing the whole deployment down in the process.
#
# Since not all the `service-template` derived deployments use the same command
# to fetch secrets, that field is customizable to fit the needs of all.
failOnMissingLocalSecrets:
  enabled: false
  secretsFetch:
    command: ["yarn"]
    args: ["secrets:fetch"]
